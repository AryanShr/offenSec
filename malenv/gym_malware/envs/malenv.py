import random
import gym
from collections import OrderedDict
from gym import spaces, utils
from gym.utils import seeding
import hashlib
import os

import numpy as np
from gym_malware.envs.utils import interface, pefeatures
import glob

from gym_malware.envs.controls import manipulate as manipulate
import time
ACTION_LOOKUP = {i: act for i, act in enumerate(
    manipulate.ACTION_TABLE.keys())}

# change this to function to the AV engine to attack
# function should be of the form
# def label_function( bytez ):
#    # returns 0.0 if benign else 1.0 if malware
label_function = interface.get_label_local
score_function = interface.get_score_local


class MalwareEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, random_sample=True, maxturns=3, output_path='evaded/blackbox/'):
        self.action_space = spaces.Discrete(len(ACTION_LOOKUP))
        self.maxturns = maxturns
        self.random_sample = random_sample
        self.sample_iteration_index = 0
        self.output_path = output_path
        self.bytesEntropy = pefeatures.PEFeatureExtractor()
        if not os.path.exists(output_path):
            os.makedirs(output_path)

        self.history = OrderedDict()

        self.samples = {}
        # with open(random.choice(glob.glob("/media/nerdcoder/New Volume/Major Project/Pesiduos/Pesidious/Data/malware/*")),'rb') as f:
        with open(random.choice(glob.glob("\\\\vboxsvr\Major_Project\malenv\Data\malware\*")),'rb') as f:
            self.bytez = f.read()
        self.observation_space = self.bytesEntropy.extract(self.bytez)
        self.prevScore = 1      
        self.last_action = None
        self.consecutive_action_count = 0


    # def step(self, action_index):
    #     self.turns += 1
    #     self.take_action(action_index) # update self.bytez
    #     episode_over = False
    #     # get reward
    #     try:
    #         self.label = label_function(self.bytez)
    #     except interface.ClassificationFailure:
    #         print("Failed to classify file")
    #         episode_over = True
    #     else:
    #         self.observation_space = self.bytesEntropy.extract(self.bytez)
    #         if self.label == 0:
    #             # we win!
    #             reward = 10.0 # !! a strong reward
    #             episode_over = True
    #             with open( os.path.join( self.output_path, self.sample_name.split('\\')[-1]), 'wb') as outfile:
    #                 outfile.write( self.bytez )
    #                 print("Wrote file to {}".format( os.path.join( self.output_path, self.sample_name.split('\\')[-1])))
    #         else:
    #             reward = 0.0
    #             episode_over = False
            

    #     if episode_over:
    #         pass
    #         #print("episode is over: reward = {}!".format(reward))

    #     return self.observation_space, reward, episode_over, ACTION_LOOKUP[action_index]
    
    def step(self, action_index):
        self.turns += 1
        self.take_action(action_index) # update self.bytez
        episode_over = False
        reward = 0
        # get reward
        try:
            stime = time.time()
            self.score = score_function(self.bytez)
            etime = time.time()
            print("Time elapsed for score", etime-stime)
        except Exception as e:
            print("Failed to classify file ", e)
            if self.turns >1:
                reward -= 10
            else:
                reward = 0
            episode_over = True
        else:
            self.observation_space = self.bytesEntropy.extract(self.bytez)
            print(self.prevScore -self.score)
            if self.score < self.prevScore:
                reward = (self.prevScore - self.score) * 10
                # Reset the consecutive action count since the action improved the score
                self.last_action = action_index
                self.consecutive_action_count = 0
            elif self.score == self.prevScore:
                reward = 0
                reward = -0.01 
            else:
            #     # Check if the last action is the same as the current action
                if self.last_action == action_index:
            #         # Increase the count of consecutive actions
                    self.consecutive_action_count += 1
                    reward = -0.1 * self.consecutive_action_count
                else:
            #         # Reset the consecutive action count and update the last action
                    self.last_action = action_index
                    self.consecutive_action_count = 0
                    reward = -1*(self.score - self.prevScore)
                    
            # else:
            #     reward = 0
            
            self.prevScore = self.score

            if self.score <= 0.6:
                reward = 10.0
                episode_over = True
                with open( os.path.join( self.output_path, self.sample_name.split('\\')[-1]), 'wb') as outfile:
                    outfile.write( self.bytez )
                    print("Wrote file to {}".format( os.path.join( self.output_path, self.sample_name.split('\\')[-1])))

        return self.observation_space, reward, episode_over, ACTION_LOOKUP[action_index]

    def take_action(self, action_index):
        assert action_index < len(ACTION_LOOKUP)
        action = ACTION_LOOKUP[action_index]
        self.bytez = bytes( manipulate.modify_without_breaking(self.bytez, [action]))

    def reset(self, file_name):
        self.turns = 0
        while True:
            # get the new environment
            if self.random_sample:
                self.sample_name = random.choice(self.available_samples)
            else: # draw a sample at random
                self.sample_name = file_name

            self.history[self.sample_name] = {'actions': [], 'evaded': False}
            try:
                with open(self.sample_name, 'rb') as f:
                    self.bytez = f.read()
            except interface.FileRetrievalFailure:
                print("failed fetching file")
                continue

            original_label = label_function(self.bytez)            
            if original_label == 0:
                print("Already benign")
                # skip this one, it's already benign, and the agent will learn nothing
                # continue

            #print("new sha256: {}".format(self.sha256))                

            self.observation_space = self.bytesEntropy.extract(self.bytez)

            break  # we're done here

        return np.asarray(self.observation_space)

    def render(self, mode='human', close=False):
        pass